{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Formatter\n",
    "----\n",
    "In this notebook we'll process the raw CI log data into a format we can plug into the neural net. Specifically, we're going to take the various nested fields of the data, flatten it, map the dataset into a list of strings, and create a 35-dimensional vector representation of that list. We'll then pop those representations out into .npy files which the neural net notebook (neuralCI.ipynb) will be using. This notebook takes a little while to run (specifically, the dataset sampling section) but once it's run once, it shouldn't need to be run again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Spark Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import pyspark\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext,Row\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "\n",
    "#stop weird parquet warns from cluttering logs\n",
    "logger = spark._jvm.org.apache.log4j\n",
    "logger.LogManager.getLogger(\"org\"). setLevel( logger.Level.ERROR )\n",
    "logger.LogManager.getLogger(\"akka\").setLevel( logger.Level.ERROR )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CI Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "data = spark.read.parquet(\"{}/rhci-moby.parquet\".format(cwd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flatten Data\n",
    "The dataset comes with a very nested schema, which for our purposes is a hindrance. Let's flatten out the dataset. Annoyingly, there's no apparent built-in way to do this in Spark, so we have to write a bit of hacky code to do it. \n",
    "\n",
    "Spark lets you select and rename columns via:\n",
    "\n",
    "`df.select(df[n_1].alias(a_1),...,df[n_n].alias(a_n))`\n",
    "\n",
    "which means that we can select a nested field (say, a.b.c) and give it a top layer alias (a_b_c), it'll appear with other top layer fields. Therefore, if we can walk through the schema, find all subfields, give them top layer aliases, and then write some giant `select` statement to grab the 59 or so different fields, we can flatten our dataset.\n",
    "\n",
    "However, writing a giant select statement by hand isn't exactly an elegant solution, so I wrote code that writes said giant select statement for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create alias for schemas\n",
    "def addAlias(string):\n",
    "    labelString = string.replace(\".\",\"_\")\n",
    "    return \"data['{}'].alias('{}')\".format(string,labelString)\n",
    "\n",
    "#recursively walk through schema and flatten, assign aliases\n",
    "def flattenSchema(schema,label):\n",
    "    global selector\n",
    "    for field in schema:\n",
    "        if isinstance(field.dataType,pyspark.sql.types.StructType):\n",
    "            flattenSchema(field.dataType,label+field.name+\".\")\n",
    "        else:   \n",
    "            if field.name != None:\n",
    "                selector.append(addAlias(label+field.name))\n",
    "\n",
    "#using array of flat, aliased schemas, generate code that will flatten the dataframe\n",
    "def createFlatFunc():\n",
    "    global selector \n",
    "    selector = []\n",
    "    flattenSchema(data.schema,\"\")\n",
    "    flattener = \",\".join(selector)\n",
    "    return \"flatData = data.select({})\".format(flattener)\n",
    "\n",
    "#execute the generated code\n",
    "exec(createFlatFunc())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample the Dataset\n",
    "We don't need the full 220,000,000 points for our model. Let's shoot for ~100k points, 75k for training, 25k for testing. (Caching here takes a bit, but saves time later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100326"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampData = flatData.sample(False, 0.00045).cache()\n",
    "sampData.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stringify Data\n",
    "The way we're going to attempt to model categorical data (i.e., almost all of the data) is by Word2Vec, so we need to turn each data point into a list of words. The simplest way to do this is to just cast each field of the data into string, but there's some unicode weirdness in the data, so we have to write function that's a bit more complex to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def unicodeToString(row):\n",
    "    ci_status = 1 if row[9]==\"SUCCESS\" else 0\n",
    "    sentence = []\n",
    "    for f,field in enumerate(row):\n",
    "        if f == 9:\n",
    "            continue\n",
    "        elif isinstance(field,list):\n",
    "            for item in field:\n",
    "                sentence.append(str(item))\n",
    "        else:\n",
    "            if type(field)==type(u'unicode'):\n",
    "                sentence.append(str(field.encode(\"utf8\")))\n",
    "            else:\n",
    "                sentence.append(str(field))\n",
    "    return Row(sentences=sentence,ci_status=ci_status)\n",
    "\n",
    "strData = spark.createDataFrame(sampData.rdd.map(lambda x: unicodeToString(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data into Training and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75176"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train,valid = strData.randomSplit((.75,.25))\n",
    "train.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Word2Vec Representation of Data\n",
    "We want to make sure to train the word2Vec model on only the training data, obviously!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "\n",
    "word2vec = Word2Vec(vectorSize=35, minCount=0,inputCol='sentences',outputCol='result')\n",
    "model = word2vec.fit(train)\n",
    "\n",
    "trainRes = model.transform(train)\n",
    "validRes = model.transform(valid)\n",
    "\n",
    "tVectors  = trainRes.collect()\n",
    "vVectors  = validRes.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Save and Write Vectors as Numpy Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.01197636  0.10916476 -0.124254   ...,  0.13654004  0.04205332\n",
      "  -0.0352273 ]\n",
      " [ 0.02029024  0.12202831 -0.12601285 ...,  0.12958692  0.04629133\n",
      "  -0.04605272]\n",
      " [ 0.01799032  0.11729641 -0.12546606 ...,  0.13338823  0.04218564\n",
      "  -0.04517513]\n",
      " ..., \n",
      " [-0.06852289  0.10906283 -0.08026791 ...,  0.11507573 -0.02935873\n",
      "  -0.06058261]\n",
      " [-0.10010055  0.18701809 -0.10264427 ...,  0.12897698  0.0058912\n",
      "   0.01007479]\n",
      " [-0.09558322  0.17580825 -0.10442847 ...,  0.12370794  0.0159801\n",
      "  -0.01374805]]\n"
     ]
    }
   ],
   "source": [
    "def numpify(data,label):\n",
    "    status,sentences,vectors = zip(*data)\n",
    "    X,Y = np.array(vectors),np.array(status)\n",
    "        \n",
    "    np.save(cwd+\"/formattedData/\"+label+\"vectors.npy\",X)\n",
    "    np.save(cwd+\"/formattedData/\"+label+\"status.npy\",Y)\n",
    "    return X,Y\n",
    "\n",
    "numpify(tVectors,\"t\")\n",
    "numpify(vVectors,\"v\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
